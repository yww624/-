1. log-cosh loss function : 
```
def logcosh(true, pred):
    loss = np.log(np.cosh(pred - true))return np.sum(loss)
    # loss = torch.log(torch.cosh()) return torch.sum(loss)
```
2. RMSprop 优化器
3. batch size ： 512
4. learning rates: $10^{-4}~10^{-8}$
5. Xavier normal initializer  and zero-initialized biases.
6. 7层gru, unit数目为：2,128,256,512,256,128,1; time_step 为：1024,512,256,128,256,512,1024;   
7. 双向gru